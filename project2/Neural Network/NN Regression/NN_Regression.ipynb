{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd7dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa74d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d143e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  tax  ptratio   \n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900  296     15.3  \\\n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 1\n",
    "df = pd.read_csv(\"Boston1.csv\")\n",
    "df = df.loc[:, ~df.columns.isin(['rad','Unnamed: 0'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e343fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('medv',axis=1).copy()\n",
    "y = df['medv'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553a95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bb0be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 23:41:00.676463: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 35ms/step - loss: 13681.8545 - mae: 101.8620 - val_loss: 16764.0781 - val_mae: 112.3103\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13388.4443 - mae: 100.5048 - val_loss: 16486.1816 - val_mae: 111.0708\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13117.9463 - mae: 99.2619 - val_loss: 16223.1523 - val_mae: 109.8356\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12861.9619 - mae: 97.9941 - val_loss: 15973.7021 - val_mae: 108.6080\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12603.2900 - mae: 96.7111 - val_loss: 15712.2725 - val_mae: 107.4286\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12354.3301 - mae: 95.5288 - val_loss: 15443.5596 - val_mae: 106.2743\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 12115.8418 - mae: 94.4432 - val_loss: 15162.1484 - val_mae: 105.1678\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11885.2832 - mae: 93.3631 - val_loss: 14904.1514 - val_mae: 104.0073\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11655.9736 - mae: 92.2760 - val_loss: 14626.2129 - val_mae: 102.9012\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11419.9727 - mae: 91.2264 - val_loss: 14361.7402 - val_mae: 101.7983\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 11200.0273 - mae: 90.2220 - val_loss: 14088.6777 - val_mae: 100.7320\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10972.2471 - mae: 89.2005 - val_loss: 13830.7881 - val_mae: 99.6464\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10756.9053 - mae: 88.1949 - val_loss: 13565.5146 - val_mae: 98.5741\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 10546.1797 - mae: 87.2324 - val_loss: 13291.4229 - val_mae: 97.5269\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 10323.1719 - mae: 86.2736 - val_loss: 13031.7969 - val_mae: 96.4796\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10119.3506 - mae: 85.3456 - val_loss: 12763.5469 - val_mae: 95.4508\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9910.0479 - mae: 84.4012 - val_loss: 12504.7695 - val_mae: 94.4125\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 9699.4805 - mae: 83.4346 - val_loss: 12263.1113 - val_mae: 93.3619\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9498.1123 - mae: 82.4807 - val_loss: 12010.6738 - val_mae: 92.3517\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9301.1836 - mae: 81.5651 - val_loss: 11761.4844 - val_mae: 91.3251\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9104.5635 - mae: 80.6633 - val_loss: 11509.2148 - val_mae: 90.3380\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8916.2881 - mae: 79.7846 - val_loss: 11258.7812 - val_mae: 89.3416\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8720.3760 - mae: 78.8623 - val_loss: 11024.5459 - val_mae: 88.3209\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8529.0557 - mae: 77.9228 - val_loss: 10801.8682 - val_mae: 87.3031\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 8354.5752 - mae: 77.0100 - val_loss: 10568.5234 - val_mae: 86.2908\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 8169.2612 - mae: 76.1083 - val_loss: 10338.0254 - val_mae: 85.3154\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7987.6104 - mae: 75.2103 - val_loss: 10128.7871 - val_mae: 84.3087\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7825.1938 - mae: 74.3288 - val_loss: 9895.4531 - val_mae: 83.3539\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7644.6631 - mae: 73.4562 - val_loss: 9683.0566 - val_mae: 82.3755\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7478.8760 - mae: 72.5807 - val_loss: 9467.9238 - val_mae: 81.4153\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7305.7080 - mae: 71.6847 - val_loss: 9266.7598 - val_mae: 80.4630\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7147.2656 - mae: 70.8328 - val_loss: 9065.2129 - val_mae: 79.5017\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6990.1187 - mae: 69.9774 - val_loss: 8858.2900 - val_mae: 78.5654\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6832.0552 - mae: 69.1331 - val_loss: 8658.0293 - val_mae: 77.6196\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6677.1040 - mae: 68.2905 - val_loss: 8460.8721 - val_mae: 76.6916\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6521.1387 - mae: 67.4273 - val_loss: 8276.9863 - val_mae: 75.7494\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6375.7085 - mae: 66.6031 - val_loss: 8085.7686 - val_mae: 74.8385\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6228.9141 - mae: 65.7641 - val_loss: 7903.7056 - val_mae: 73.9084\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6084.2651 - mae: 64.9424 - val_loss: 7721.2993 - val_mae: 73.0104\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5943.3135 - mae: 64.1138 - val_loss: 7546.2466 - val_mae: 72.0964\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5803.9639 - mae: 63.2892 - val_loss: 7375.6309 - val_mae: 71.1946\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5669.4932 - mae: 62.4860 - val_loss: 7202.2085 - val_mae: 70.3191\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5542.3457 - mae: 61.7096 - val_loss: 7028.5708 - val_mae: 69.4282\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5406.8311 - mae: 60.8990 - val_loss: 6864.6216 - val_mae: 68.5451\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5277.0532 - mae: 60.1028 - val_loss: 6707.5630 - val_mae: 67.6761\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5156.6631 - mae: 59.3375 - val_loss: 6544.2969 - val_mae: 66.8252\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5030.8306 - mae: 58.5632 - val_loss: 6390.5640 - val_mae: 65.9696\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4914.1318 - mae: 57.7823 - val_loss: 6237.3999 - val_mae: 65.1008\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4794.2109 - mae: 57.0101 - val_loss: 6088.4614 - val_mae: 64.2583\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4678.4609 - mae: 56.2494 - val_loss: 5943.4980 - val_mae: 63.4140\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4569.5762 - mae: 55.5123 - val_loss: 5795.7480 - val_mae: 62.5904\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4454.0454 - mae: 54.7521 - val_loss: 5660.2266 - val_mae: 61.7668\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4346.5249 - mae: 54.0082 - val_loss: 5525.4175 - val_mae: 60.9604\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4240.4062 - mae: 53.2821 - val_loss: 5395.5186 - val_mae: 60.1593\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4138.4580 - mae: 52.5464 - val_loss: 5265.5444 - val_mae: 59.3646\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4038.8142 - mae: 51.8257 - val_loss: 5136.0405 - val_mae: 58.5706\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3943.6194 - mae: 51.1233 - val_loss: 5005.7056 - val_mae: 57.7798\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3842.1040 - mae: 50.4035 - val_loss: 4883.6685 - val_mae: 57.0000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3749.5723 - mae: 49.6948 - val_loss: 4763.9302 - val_mae: 56.2166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3655.7979 - mae: 48.9859 - val_loss: 4646.6426 - val_mae: 55.4530\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3566.3271 - mae: 48.2980 - val_loss: 4532.2886 - val_mae: 54.6897\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3475.7002 - mae: 47.6082 - val_loss: 4423.9487 - val_mae: 53.9413\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3390.2427 - mae: 46.9436 - val_loss: 4314.4927 - val_mae: 53.2106\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3308.3020 - mae: 46.3280 - val_loss: 4203.2847 - val_mae: 52.5056\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3224.7573 - mae: 45.6766 - val_loss: 4102.0210 - val_mae: 51.7923\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3143.2268 - mae: 45.0225 - val_loss: 4002.7993 - val_mae: 51.0964\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3065.5527 - mae: 44.4080 - val_loss: 3900.2910 - val_mae: 50.4154\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2990.6069 - mae: 43.8171 - val_loss: 3799.3711 - val_mae: 49.7287\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2917.7913 - mae: 43.2314 - val_loss: 3698.8623 - val_mae: 49.0485\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2841.4146 - mae: 42.6378 - val_loss: 3604.6646 - val_mae: 48.3894\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2769.4248 - mae: 42.0422 - val_loss: 3514.4731 - val_mae: 47.7335\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2700.3301 - mae: 41.4538 - val_loss: 3425.4187 - val_mae: 47.0877\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2631.9470 - mae: 40.8821 - val_loss: 3340.0176 - val_mae: 46.4443\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2567.4285 - mae: 40.3301 - val_loss: 3253.9180 - val_mae: 45.8015\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2500.0911 - mae: 39.7566 - val_loss: 3173.0164 - val_mae: 45.1762\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2437.3230 - mae: 39.2230 - val_loss: 3093.4758 - val_mae: 44.5573\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2375.6255 - mae: 38.6869 - val_loss: 3017.0183 - val_mae: 43.9495\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2318.6196 - mae: 38.1708 - val_loss: 2937.6011 - val_mae: 43.3353\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2257.9099 - mae: 37.6426 - val_loss: 2860.9248 - val_mae: 42.7383\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2200.9163 - mae: 37.1368 - val_loss: 2786.3652 - val_mae: 42.1510\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2147.4131 - mae: 36.6577 - val_loss: 2711.5522 - val_mae: 41.5725\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2090.0105 - mae: 36.1308 - val_loss: 2644.1343 - val_mae: 41.0208\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2038.3304 - mae: 35.6410 - val_loss: 2576.3462 - val_mae: 40.4723\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1987.0479 - mae: 35.1565 - val_loss: 2509.5798 - val_mae: 39.9315\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1935.7513 - mae: 34.6665 - val_loss: 2446.6995 - val_mae: 39.4033\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1887.1300 - mae: 34.1958 - val_loss: 2384.1089 - val_mae: 38.8846\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1839.8547 - mae: 33.7465 - val_loss: 2322.6482 - val_mae: 38.3701\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1793.4473 - mae: 33.3041 - val_loss: 2262.9089 - val_mae: 37.8616\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1748.7346 - mae: 32.8720 - val_loss: 2205.2244 - val_mae: 37.3535\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1704.5635 - mae: 32.4403 - val_loss: 2146.6948 - val_mae: 36.8570\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1663.6814 - mae: 32.0278 - val_loss: 2089.8799 - val_mae: 36.3578\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1618.3496 - mae: 31.5961 - val_loss: 2037.8853 - val_mae: 35.8848\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1579.9333 - mae: 31.1959 - val_loss: 1984.4597 - val_mae: 35.4071\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1539.7853 - mae: 30.7982 - val_loss: 1934.2628 - val_mae: 34.9372\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1502.2880 - mae: 30.4081 - val_loss: 1883.5586 - val_mae: 34.4730\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1465.4985 - mae: 30.0290 - val_loss: 1833.2367 - val_mae: 34.0192\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1427.4640 - mae: 29.6378 - val_loss: 1787.7267 - val_mae: 33.5713\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1392.9023 - mae: 29.2566 - val_loss: 1742.3873 - val_mae: 33.1247\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1358.9691 - mae: 28.8846 - val_loss: 1697.8395 - val_mae: 32.6810\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1325.7611 - mae: 28.5147 - val_loss: 1654.4221 - val_mae: 32.2430\n"
     ]
    }
   ],
   "source": [
    "#question 2\n",
    "\n",
    "def build_nn2():\n",
    "    \n",
    "    network = Sequential()\n",
    "    network.add(Dense(1,input_dim=x_train.shape[1]))\n",
    "    network.compile(optimizer=Adam(lr=0.03),\n",
    "                   loss='mean_squared_error',\n",
    "                   metrics=['mae'])\n",
    "    return network\n",
    "    \n",
    "model2 = build_nn2()\n",
    "\n",
    "history2 = model2.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f83269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.24298095703125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred2 = model2.predict(x_test)\n",
    "mae2 = history2.history['val_mae']\n",
    "mae2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd4e6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 11ms/step - loss: 1654.4222 - mae: 32.2430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1654.4222412109375, 32.24298095703125)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mse2, test_mae2 = model2.evaluate(x_test,y_test)\n",
    "test_mse2, test_mae2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52356f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 40ms/step - loss: 582.3256 - mae: 19.2694 - val_loss: 542.9659 - val_mae: 17.6905\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 531.9144 - mae: 17.6655 - val_loss: 508.4691 - val_mae: 16.5222\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 487.4189 - mae: 16.7912 - val_loss: 468.2516 - val_mae: 15.9944\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 448.4480 - mae: 16.2968 - val_loss: 430.8941 - val_mae: 15.7018\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 412.4124 - mae: 15.9190 - val_loss: 399.8203 - val_mae: 15.3231\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 380.6212 - mae: 15.3215 - val_loss: 370.6278 - val_mae: 14.5817\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 350.6417 - mae: 14.6112 - val_loss: 344.8603 - val_mae: 14.0390\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 326.5168 - mae: 14.0619 - val_loss: 322.0283 - val_mae: 13.5361\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 304.9504 - mae: 13.6516 - val_loss: 301.2366 - val_mae: 13.2739\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 284.4307 - mae: 13.2575 - val_loss: 282.8634 - val_mae: 12.8107\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 266.1511 - mae: 12.7770 - val_loss: 266.8382 - val_mae: 12.3756\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 249.8885 - mae: 12.4080 - val_loss: 251.7897 - val_mae: 12.1388\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 234.8764 - mae: 12.1281 - val_loss: 238.4361 - val_mae: 11.9021\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 222.0091 - mae: 11.8432 - val_loss: 225.8058 - val_mae: 11.6187\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 209.7757 - mae: 11.5214 - val_loss: 213.9377 - val_mae: 11.2744\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 198.6731 - mae: 11.1839 - val_loss: 202.6820 - val_mae: 10.9340\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 187.9650 - mae: 10.8222 - val_loss: 192.6633 - val_mae: 10.6566\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 178.6175 - mae: 10.6168 - val_loss: 184.0337 - val_mae: 10.5387\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 169.9136 - mae: 10.3950 - val_loss: 175.1376 - val_mae: 10.2759\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 161.5028 - mae: 10.1337 - val_loss: 167.0899 - val_mae: 10.0512\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 154.0567 - mae: 9.9139 - val_loss: 159.5787 - val_mae: 9.8372\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 146.4110 - mae: 9.6267 - val_loss: 152.1284 - val_mae: 9.5444\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 140.1128 - mae: 9.3730 - val_loss: 145.7542 - val_mae: 9.3791\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 134.2438 - mae: 9.2161 - val_loss: 140.1435 - val_mae: 9.2625\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 128.1528 - mae: 9.0367 - val_loss: 134.3911 - val_mae: 9.0589\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 123.3441 - mae: 8.8070 - val_loss: 128.5874 - val_mae: 8.8410\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 118.4462 - mae: 8.5928 - val_loss: 123.2228 - val_mae: 8.6142\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 113.6170 - mae: 8.4559 - val_loss: 119.8916 - val_mae: 8.6051\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 109.2995 - mae: 8.3258 - val_loss: 115.2097 - val_mae: 8.4320\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 105.2711 - mae: 8.1301 - val_loss: 110.7165 - val_mae: 8.2353\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 101.8677 - mae: 7.9352 - val_loss: 106.2396 - val_mae: 8.0322\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 98.3868 - mae: 7.7859 - val_loss: 103.2619 - val_mae: 7.9583\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 95.5252 - mae: 7.7068 - val_loss: 100.8735 - val_mae: 7.8784\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 92.6459 - mae: 7.5434 - val_loss: 96.9823 - val_mae: 7.6711\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 90.1160 - mae: 7.3691 - val_loss: 93.4646 - val_mae: 7.4695\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 88.1504 - mae: 7.3374 - val_loss: 92.7324 - val_mae: 7.5293\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 85.6383 - mae: 7.2182 - val_loss: 89.1369 - val_mae: 7.2811\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 83.8052 - mae: 7.0856 - val_loss: 86.8047 - val_mae: 7.1375\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 82.0838 - mae: 6.9885 - val_loss: 85.9594 - val_mae: 7.1200\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 80.7660 - mae: 7.0013 - val_loss: 84.9325 - val_mae: 7.1137\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 79.0785 - mae: 6.9140 - val_loss: 81.5315 - val_mae: 6.8692\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 77.9463 - mae: 6.7771 - val_loss: 79.5785 - val_mae: 6.7434\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 77.2234 - mae: 6.7760 - val_loss: 79.8382 - val_mae: 6.8696\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 76.0792 - mae: 6.7122 - val_loss: 77.3062 - val_mae: 6.6550\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 75.1812 - mae: 6.6690 - val_loss: 77.5702 - val_mae: 6.7537\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 74.7382 - mae: 6.6102 - val_loss: 75.6805 - val_mae: 6.5634\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 73.9606 - mae: 6.6170 - val_loss: 77.2397 - val_mae: 6.7926\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 73.1096 - mae: 6.6315 - val_loss: 74.3016 - val_mae: 6.5884\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 73.0505 - mae: 6.5103 - val_loss: 72.1289 - val_mae: 6.3862\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 72.1777 - mae: 6.5365 - val_loss: 73.8780 - val_mae: 6.6964\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 71.7607 - mae: 6.6146 - val_loss: 73.1904 - val_mae: 6.5932\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 71.1965 - mae: 6.4996 - val_loss: 72.2483 - val_mae: 6.4960\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 70.7121 - mae: 6.4177 - val_loss: 70.7302 - val_mae: 6.3933\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 70.4934 - mae: 6.4486 - val_loss: 71.2731 - val_mae: 6.5610\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 70.1068 - mae: 6.4653 - val_loss: 70.0349 - val_mae: 6.4736\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 69.7083 - mae: 6.4291 - val_loss: 69.5652 - val_mae: 6.4204\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 69.8816 - mae: 6.3560 - val_loss: 68.2940 - val_mae: 6.2482\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 69.7348 - mae: 6.4404 - val_loss: 71.7390 - val_mae: 6.6980\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 69.1101 - mae: 6.4559 - val_loss: 68.4437 - val_mae: 6.3509\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 68.7192 - mae: 6.3915 - val_loss: 68.2607 - val_mae: 6.4154\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 68.1991 - mae: 6.3453 - val_loss: 67.6028 - val_mae: 6.3511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 68.0609 - mae: 6.2947 - val_loss: 67.6991 - val_mae: 6.3618\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 67.9252 - mae: 6.3838 - val_loss: 69.2271 - val_mae: 6.5696\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 68.3267 - mae: 6.3400 - val_loss: 66.7889 - val_mae: 6.2252\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 67.4732 - mae: 6.3065 - val_loss: 67.8351 - val_mae: 6.4742\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 67.0533 - mae: 6.3553 - val_loss: 66.8283 - val_mae: 6.3924\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66.7845 - mae: 6.2808 - val_loss: 66.1636 - val_mae: 6.2986\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 66.6359 - mae: 6.2163 - val_loss: 66.2078 - val_mae: 6.2808\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 66.6584 - mae: 6.3102 - val_loss: 68.0685 - val_mae: 6.5174\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 66.4507 - mae: 6.2537 - val_loss: 64.7566 - val_mae: 6.1483\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 65.8618 - mae: 6.2053 - val_loss: 66.2623 - val_mae: 6.3788\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 65.7546 - mae: 6.2371 - val_loss: 65.0752 - val_mae: 6.2872\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 65.8606 - mae: 6.3008 - val_loss: 66.4925 - val_mae: 6.4009\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 65.0801 - mae: 6.2290 - val_loss: 64.5958 - val_mae: 6.2124\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 65.1114 - mae: 6.1245 - val_loss: 63.2574 - val_mae: 6.0780\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 64.7065 - mae: 6.1209 - val_loss: 65.0549 - val_mae: 6.3171\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 65.0634 - mae: 6.3028 - val_loss: 66.2074 - val_mae: 6.4585\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 64.2854 - mae: 6.1568 - val_loss: 63.4610 - val_mae: 6.1140\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 64.6331 - mae: 6.0509 - val_loss: 62.8945 - val_mae: 6.0424\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 63.6798 - mae: 6.0962 - val_loss: 64.9314 - val_mae: 6.3580\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 63.9830 - mae: 6.2184 - val_loss: 64.3487 - val_mae: 6.3099\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 63.8764 - mae: 6.2101 - val_loss: 63.9113 - val_mae: 6.2560\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 63.6885 - mae: 6.0565 - val_loss: 61.2649 - val_mae: 5.9487\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 63.0076 - mae: 6.0138 - val_loss: 63.4799 - val_mae: 6.2357\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 63.0770 - mae: 6.1218 - val_loss: 63.7677 - val_mae: 6.2512\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 62.9063 - mae: 6.1408 - val_loss: 63.2903 - val_mae: 6.2467\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 62.5701 - mae: 6.0574 - val_loss: 61.2723 - val_mae: 6.0244\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 62.2580 - mae: 5.9931 - val_loss: 62.4036 - val_mae: 6.1436\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 62.4477 - mae: 6.1152 - val_loss: 63.7227 - val_mae: 6.3064\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 61.7687 - mae: 6.0519 - val_loss: 61.0828 - val_mae: 6.0246\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 61.8807 - mae: 5.9425 - val_loss: 60.6060 - val_mae: 5.9727\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 61.7416 - mae: 6.0318 - val_loss: 63.0476 - val_mae: 6.2923\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 61.5923 - mae: 6.0242 - val_loss: 61.1565 - val_mae: 6.0100\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 61.2928 - mae: 5.9300 - val_loss: 60.8237 - val_mae: 6.0590\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 60.8225 - mae: 5.9839 - val_loss: 61.6544 - val_mae: 6.1670\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 60.8666 - mae: 5.9724 - val_loss: 60.6715 - val_mae: 6.0804\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 60.5365 - mae: 5.9791 - val_loss: 61.0174 - val_mae: 6.0979\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 60.3798 - mae: 5.9374 - val_loss: 60.5766 - val_mae: 6.0431\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 60.6959 - mae: 5.9942 - val_loss: 61.2223 - val_mae: 6.0888\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 60.2506 - mae: 5.9150 - val_loss: 60.2053 - val_mae: 6.0189\n"
     ]
    }
   ],
   "source": [
    "#question 3\n",
    "def build_nn3(hidden_layers,neurons):\n",
    "    \n",
    "    net = Sequential()\n",
    "    for i in range(hidden_layers+1):\n",
    "        # Add fully connected layer with ReLu activation\n",
    "        net.add(Dense(\n",
    "            neurons, \n",
    "            input_dim=x_train.shape[1],\n",
    "            ))\n",
    "        # Add droput layer\n",
    "        net.add(Dropout(0))\n",
    "    net.add(Dense(1,input_dim=x_train.shape[1])) \n",
    "    opt = Adam(lr=0.03)\n",
    "    net.compile(optimizer=Adam(lr=0.03),\n",
    "                   loss='mean_squared_error',\n",
    "                   metrics=['mae'])\n",
    "    return net\n",
    "\n",
    "model3 = build_nn3(1,3)\n",
    "\n",
    "history3 = model3.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b9b5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 14ms/step - loss: 60.2053 - mae: 6.0189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60.205291748046875, 6.018930435180664)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mse3, test_mae3 = model3.evaluate(x_test,y_test)\n",
    "test_mse3, test_mae3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff72cbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.871005067365086"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question 4\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model4 = LinearRegression()\n",
    "model4.fit(x_train,y_train)\n",
    "y_pred4 = model4.predict(x_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test,y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9332a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1162.7335 - mae: 32.3652 - val_loss: 980.6610 - val_mae: 29.8107\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1067.5194 - mae: 30.9262 - val_loss: 899.5001 - val_mae: 28.4831\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 983.8203 - mae: 29.6251 - val_loss: 833.4846 - val_mae: 27.3310\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 916.2145 - mae: 28.4948 - val_loss: 781.0598 - val_mae: 26.3598\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 862.4731 - mae: 27.5719 - val_loss: 738.7625 - val_mae: 25.5364\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 818.5113 - mae: 26.7717 - val_loss: 703.9984 - val_mae: 24.8369\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 782.9232 - mae: 26.1124 - val_loss: 675.7657 - val_mae: 24.2554\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 754.2149 - mae: 25.5678 - val_loss: 652.5270 - val_mae: 23.7733\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 731.3593 - mae: 25.1281 - val_loss: 633.2809 - val_mae: 23.3805\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 712.9767 - mae: 24.7656 - val_loss: 616.4811 - val_mae: 23.0444\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 696.7902 - mae: 24.4556 - val_loss: 602.3436 - val_mae: 22.7636\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 683.1893 - mae: 24.1940 - val_loss: 590.3553 - val_mae: 22.5290\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 671.6721 - mae: 23.9763 - val_loss: 580.0443 - val_mae: 22.3291\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 662.0085 - mae: 23.7936 - val_loss: 571.1364 - val_mae: 22.1623\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 653.5604 - mae: 23.6393 - val_loss: 563.7625 - val_mae: 22.0209\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 646.6047 - mae: 23.5066 - val_loss: 557.8416 - val_mae: 21.9028\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 641.0759 - mae: 23.3992 - val_loss: 552.8887 - val_mae: 21.8041\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 636.3726 - mae: 23.3072 - val_loss: 548.9328 - val_mae: 21.7246\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 632.5096 - mae: 23.2326 - val_loss: 545.6596 - val_mae: 21.6638\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 629.1631 - mae: 23.1712 - val_loss: 542.8837 - val_mae: 21.6139\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 626.2182 - mae: 23.1170 - val_loss: 540.3069 - val_mae: 21.5684\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 623.4537 - mae: 23.0657 - val_loss: 538.0840 - val_mae: 21.5280\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 621.0702 - mae: 23.0212 - val_loss: 536.0895 - val_mae: 21.4922\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 618.9980 - mae: 22.9824 - val_loss: 534.3049 - val_mae: 21.4617\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 617.2048 - mae: 22.9495 - val_loss: 532.8207 - val_mae: 21.4362\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 615.6676 - mae: 22.9205 - val_loss: 531.5751 - val_mae: 21.4141\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 614.2333 - mae: 22.8933 - val_loss: 530.4655 - val_mae: 21.3943\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 613.0514 - mae: 22.8705 - val_loss: 529.3663 - val_mae: 21.3752\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 611.9753 - mae: 22.8510 - val_loss: 528.3420 - val_mae: 21.3572\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 611.0873 - mae: 22.8337 - val_loss: 527.4285 - val_mae: 21.3412\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 610.2107 - mae: 22.8170 - val_loss: 526.6268 - val_mae: 21.3267\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 609.4451 - mae: 22.8022 - val_loss: 525.8174 - val_mae: 21.3124\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 608.7215 - mae: 22.7886 - val_loss: 525.0577 - val_mae: 21.2986\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 608.0321 - mae: 22.7754 - val_loss: 524.2676 - val_mae: 21.2849\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 607.3816 - mae: 22.7632 - val_loss: 523.5080 - val_mae: 21.2715\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 606.7507 - mae: 22.7514 - val_loss: 522.8004 - val_mae: 21.2588\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 606.1703 - mae: 22.7402 - val_loss: 522.1087 - val_mae: 21.2464\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 605.6357 - mae: 22.7302 - val_loss: 521.4117 - val_mae: 21.2339\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 605.0601 - mae: 22.7191 - val_loss: 520.8265 - val_mae: 21.2226\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 604.5700 - mae: 22.7094 - val_loss: 520.2624 - val_mae: 21.2117\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 604.0634 - mae: 22.6994 - val_loss: 519.7569 - val_mae: 21.2016\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 603.6539 - mae: 22.6910 - val_loss: 519.2964 - val_mae: 21.1923\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 603.2711 - mae: 22.6830 - val_loss: 518.8755 - val_mae: 21.1836\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 602.9145 - mae: 22.6756 - val_loss: 518.4709 - val_mae: 21.1755\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 602.5753 - mae: 22.6686 - val_loss: 518.1260 - val_mae: 21.1683\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 602.2610 - mae: 22.6619 - val_loss: 517.8547 - val_mae: 21.1620\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 601.9705 - mae: 22.6556 - val_loss: 517.5922 - val_mae: 21.1558\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 601.6833 - mae: 22.6494 - val_loss: 517.3336 - val_mae: 21.1497\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 601.4002 - mae: 22.6433 - val_loss: 517.0800 - val_mae: 21.1437\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 601.1219 - mae: 22.6372 - val_loss: 516.8275 - val_mae: 21.1378\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 600.8487 - mae: 22.6313 - val_loss: 516.5733 - val_mae: 21.1317\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 600.5801 - mae: 22.6253 - val_loss: 516.3224 - val_mae: 21.1258\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 600.3087 - mae: 22.6194 - val_loss: 516.0758 - val_mae: 21.1200\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 600.0370 - mae: 22.6134 - val_loss: 515.8284 - val_mae: 21.1141\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 599.7707 - mae: 22.6075 - val_loss: 515.5760 - val_mae: 21.1081\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 599.5024 - mae: 22.6016 - val_loss: 515.3262 - val_mae: 21.1022\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 599.2380 - mae: 22.5957 - val_loss: 515.0790 - val_mae: 21.0963\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 598.9756 - mae: 22.5899 - val_loss: 514.8332 - val_mae: 21.0905\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 598.7156 - mae: 22.5841 - val_loss: 514.5883 - val_mae: 21.0847\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 598.4471 - mae: 22.5782 - val_loss: 514.3454 - val_mae: 21.0789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 598.1877 - mae: 22.5725 - val_loss: 514.1052 - val_mae: 21.0732\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 597.9258 - mae: 22.5667 - val_loss: 513.8606 - val_mae: 21.0674\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 597.6625 - mae: 22.5609 - val_loss: 513.6177 - val_mae: 21.0616\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 597.4020 - mae: 22.5551 - val_loss: 513.3727 - val_mae: 21.0558\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 597.1419 - mae: 22.5493 - val_loss: 513.1317 - val_mae: 21.0501\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 596.8772 - mae: 22.5434 - val_loss: 512.8861 - val_mae: 21.0442\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 596.6157 - mae: 22.5376 - val_loss: 512.6427 - val_mae: 21.0385\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 596.3562 - mae: 22.5318 - val_loss: 512.3969 - val_mae: 21.0326\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 596.0904 - mae: 22.5260 - val_loss: 512.1598 - val_mae: 21.0270\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 595.8331 - mae: 22.5203 - val_loss: 511.9200 - val_mae: 21.0213\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 595.5737 - mae: 22.5144 - val_loss: 511.6736 - val_mae: 21.0154\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 595.3107 - mae: 22.5086 - val_loss: 511.4312 - val_mae: 21.0096\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 595.0504 - mae: 22.5028 - val_loss: 511.1888 - val_mae: 21.0039\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 594.7888 - mae: 22.4971 - val_loss: 510.9503 - val_mae: 20.9982\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 594.5348 - mae: 22.4913 - val_loss: 510.7090 - val_mae: 20.9925\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 594.2700 - mae: 22.4855 - val_loss: 510.4626 - val_mae: 20.9866\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 594.0098 - mae: 22.4797 - val_loss: 510.2200 - val_mae: 20.9808\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 593.7504 - mae: 22.4739 - val_loss: 509.9782 - val_mae: 20.9750\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 593.4881 - mae: 22.4681 - val_loss: 509.7378 - val_mae: 20.9693\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 593.2294 - mae: 22.4623 - val_loss: 509.4974 - val_mae: 20.9636\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 592.9703 - mae: 22.4565 - val_loss: 509.2572 - val_mae: 20.9578\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 592.7084 - mae: 22.4507 - val_loss: 509.0135 - val_mae: 20.9520\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 592.4480 - mae: 22.4449 - val_loss: 508.7731 - val_mae: 20.9463\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 592.1866 - mae: 22.4391 - val_loss: 508.5303 - val_mae: 20.9405\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 591.9316 - mae: 22.4333 - val_loss: 508.2894 - val_mae: 20.9347\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 591.6674 - mae: 22.4275 - val_loss: 508.0454 - val_mae: 20.9289\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 591.4089 - mae: 22.4217 - val_loss: 507.8002 - val_mae: 20.9231\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 591.1464 - mae: 22.4159 - val_loss: 507.5593 - val_mae: 20.9173\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 590.8866 - mae: 22.4101 - val_loss: 507.3200 - val_mae: 20.9116\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 590.6248 - mae: 22.4043 - val_loss: 507.0788 - val_mae: 20.9058\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 590.3655 - mae: 22.3985 - val_loss: 506.8353 - val_mae: 20.9000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 590.1062 - mae: 22.3927 - val_loss: 506.5959 - val_mae: 20.8943\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 589.8479 - mae: 22.3868 - val_loss: 506.3526 - val_mae: 20.8884\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 589.5856 - mae: 22.3810 - val_loss: 506.1092 - val_mae: 20.8826\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 589.3259 - mae: 22.3752 - val_loss: 505.8683 - val_mae: 20.8768\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 589.0670 - mae: 22.3694 - val_loss: 505.6270 - val_mae: 20.8711\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 588.8033 - mae: 22.3636 - val_loss: 505.3842 - val_mae: 20.8652\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 588.5463 - mae: 22.3578 - val_loss: 505.1438 - val_mae: 20.8595\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 588.2855 - mae: 22.3520 - val_loss: 504.9038 - val_mae: 20.8537\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 588.0270 - mae: 22.3461 - val_loss: 504.6595 - val_mae: 20.8479\n"
     ]
    }
   ],
   "source": [
    "def build_nn5(hidden_layers,neurons):\n",
    "    \n",
    "    net = Sequential()\n",
    "    for i in range(hidden_layers+1):\n",
    "        # Add fully connected layer with ReLu activation\n",
    "        net.add(Dense(\n",
    "            neurons, \n",
    "            input_dim=x_train.shape[1],\n",
    "            activation='relu'\n",
    "            ))\n",
    "        # Add droput layer\n",
    "        net.add(Dropout(0))\n",
    "    net.add(Dense(1,input_dim=x_train.shape[1])) \n",
    "    opt = Adam(lr=0.03)\n",
    "    net.compile(optimizer=Adam(lr=0.03),\n",
    "                   loss='mean_squared_error',\n",
    "                   metrics=['mae'])\n",
    "    return net\n",
    "\n",
    "model5 = build_nn5(1,3)\n",
    "\n",
    "history5 = model5.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77a8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 12ms/step - loss: 504.6595 - mae: 20.8479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(504.65948486328125, 20.847867965698242)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mse5, test_mae5 = model5.evaluate(x_test,y_test)\n",
    "test_mse5, test_mae5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
